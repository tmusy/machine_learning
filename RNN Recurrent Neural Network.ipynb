{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea\n",
    "RNN do take in consideration the past by storing information of previous steps in distributed hidden state. This is a powerful model, which is applied when sequences of data are given. \n",
    "\n",
    "## Base Model\n",
    "A RNN is based on the concept of the autoregressive model. However it is much more elaborated by using many hidden layers and special neurons like LSTM.\n",
    "\n",
    "### Autoregressive model\n",
    "In autoregressive models we forecast the variable of interest using a linear combination of past values of the variable.\n",
    "\n",
    "Def:\n",
    "\n",
    "$ yt = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} + e_t $\n",
    "\n",
    "where c is a constant and et is white noise\n",
    "\n",
    "![autoregressive model](resources/autoregressive_model.png)\n",
    "\n",
    "### RNN Model\n",
    "![rnn model](resources/rnn_model.jpeg)\n",
    "\n",
    "## Popular Architectures\n",
    "\n",
    "## Characteristics\n",
    "* Neural Network applied on sequences\n",
    "* With enough neurons and time anything can be computed that can be computed by a computer\n",
    "* RNN tend to explode or vanish during backpropagation. There are different ways to learn RNNs:\n",
    "    * LSTM\n",
    "    * Echo State Networks\n",
    "    * Good initialization with momentum\n",
    "    * Hessian Free Optimization [Martens and Sutskever, 2011]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos\n",
    "* [Hinton course on Coursera: 1. Modeling Sequences a Brief Overview](https://www.youtube.com/watch?v=lKDfBZz7Wy8&index=1&list=PLnnr1O8OWc6YM16tj9pdhBZOS9tDktNrx)\n",
    "* [Hinton course on Coursera: 4. Why it is Difficult to Train an RNN](https://www.youtube.com/watch?v=Pp4oKq4kCYs&list=PLnnr1O8OWc6YM16tj9pdhBZOS9tDktNrx&index=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "In this notebook I will reproduce a simple example from the lasagne framework to explore the framework. https://github.com/Lasagne/Recipes/blob/master/examples/lstm_text_generation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 650M (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import urllib2 #For downloading the sample text file. You won't need this if you are providing your own file.\n",
    "\n",
    "try:\n",
    "    in_text = urllib2.urlopen('https://s3.amazonaws.com/text-datasets/nietzsche.txt').read()\n",
    "    #You can also use your own file\n",
    "    #The file must be a simple text file.\n",
    "    #Simply edit the file name below and uncomment the line.  \n",
    "    #in_text = open('your_file.txt', 'r').read()\n",
    "    in_text = in_text.decode(\"utf-8-sig\").encode(\"utf-8\")\n",
    "except Exception as e:\n",
    "    print(\"Please verify the location of the input file/URL.\")\n",
    "    print(\"A sample txt file can be downloaded from https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "    raise IOError('Unable to Read Text')\n",
    "\n",
    "generation_phrase = \"The quick brown fox jumps\" #This phrase will be used as seed to generate text.\n",
    "\n",
    "#This snippet loads the text file and creates dictionaries to \n",
    "#encode characters into a vector-space representation and vice-versa. \n",
    "chars = list(set(in_text))\n",
    "data_size, vocab_size = len(in_text), len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "#Lasagne Seed for Reproducibility\n",
    "lasagne.random.set_rng(np.random.RandomState(1))\n",
    "\n",
    "# Sequence Length\n",
    "SEQ_LENGTH = 20\n",
    "\n",
    "# Number of units in the two hidden (LSTM) layers\n",
    "N_HIDDEN = 256\n",
    "\n",
    "# Optimization learning rate\n",
    "LEARNING_RATE = .01\n",
    "\n",
    "# All gradients above this will be clipped\n",
    "GRAD_CLIP = 100\n",
    "\n",
    "# How often should we check the output?\n",
    "PRINT_FREQ = 1000\n",
    "\n",
    "# Number of epochs to train the net\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "# Batch Size\n",
    "BATCH_SIZE = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_data(p, batch_size = BATCH_SIZE, data=in_text, return_target=True):\n",
    "    '''\n",
    "    This function produces a semi-redundant batch of training samples from the location 'p' in the provided string (data).\n",
    "    For instance, assuming SEQ_LENGTH = 5 and p=0, the function would create batches of \n",
    "    5 characters of the string (starting from the 0th character and stepping by 1 for each semi-redundant batch)\n",
    "    as the input and the next character as the target.\n",
    "    To make this clear, let us look at a concrete example. Assume that SEQ_LENGTH = 5, p = 0 and BATCH_SIZE = 2\n",
    "    If the input string was \"The quick brown fox jumps over the lazy dog.\",\n",
    "    For the first data point,\n",
    "    x (the inputs to the neural network) would correspond to the encoding of 'T','h','e',' ','q'\n",
    "    y (the targets of the neural network) would be the encoding of 'u'\n",
    "    For the second point,\n",
    "    x (the inputs to the neural network) would correspond to the encoding of 'h','e',' ','q', 'u'\n",
    "    y (the targets of the neural network) would be the encoding of 'i'\n",
    "    The data points are then stacked (into a three-dimensional tensor of size (batch_size,SEQ_LENGTH,vocab_size))\n",
    "    and returned. \n",
    "    Notice that there is overlap of characters between the batches (hence the name, semi-redundant batch).\n",
    "    '''\n",
    "    x = np.zeros((batch_size,SEQ_LENGTH,vocab_size))\n",
    "    y = np.zeros(batch_size)\n",
    "\n",
    "    for n in range(batch_size):\n",
    "        ptr = n\n",
    "        for i in range(SEQ_LENGTH):\n",
    "            x[n,i,char_to_ix[data[p+ptr+i]]] = 1.\n",
    "        if(return_target):\n",
    "            y[n] = char_to_ix[data[p+ptr+SEQ_LENGTH]]\n",
    "    return x, np.array(y,dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(num_epochs=NUM_EPOCHS):\n",
    "    print(\"Building network ...\")\n",
    "   \n",
    "    # First, we build the network, starting with an input layer\n",
    "    # Recurrent layers expect input of shape\n",
    "    # (batch size, SEQ_LENGTH, num_features)\n",
    "    \n",
    "    # num_features = vocab_size = all possible chars in the collection\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, None, vocab_size))\n",
    "\n",
    "    # We now build the LSTM layer which takes l_in as the input layer\n",
    "    # We clip the gradients at GRAD_CLIP to prevent the problem of exploding gradients. \n",
    "\n",
    "    l_forward_1 = lasagne.layers.LSTMLayer(\n",
    "        l_in, N_HIDDEN, grad_clipping=GRAD_CLIP,\n",
    "        nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "    l_forward_2 = lasagne.layers.LSTMLayer(\n",
    "        l_forward_1, N_HIDDEN, grad_clipping=GRAD_CLIP,\n",
    "        nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "    # The l_forward layer creates an output of dimension (batch_size, SEQ_LENGTH, N_HIDDEN)\n",
    "    # Since we are only interested in the final prediction, we isolate that quantity and feed it to the next layer. \n",
    "    # The output of the sliced layer will then be of size (batch_size, N_HIDDEN)\n",
    "    l_forward_slice = lasagne.layers.SliceLayer(l_forward_2, -1, 1)\n",
    "\n",
    "    # The sliced output is then passed through the softmax nonlinearity to create probability distribution of the prediction\n",
    "    # The output of this stage is (batch_size, vocab_size)\n",
    "    l_out = lasagne.layers.DenseLayer(l_forward_slice, num_units=vocab_size, W = lasagne.init.Normal(), nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    # Theano tensor for the targets\n",
    "    target_values = T.ivector('target_output')\n",
    "    \n",
    "    # lasagne.layers.get_output produces a variable for the output of the net\n",
    "    network_output = lasagne.layers.get_output(l_out)\n",
    "\n",
    "    # The loss function is calculated as the mean of the (categorical) cross-entropy between the prediction and target.\n",
    "    cost = T.nnet.categorical_crossentropy(network_output,target_values).mean()\n",
    "\n",
    "    # Retrieve all parameters from the network\n",
    "    all_params = lasagne.layers.get_all_params(l_out)\n",
    "\n",
    "    # Compute AdaGrad updates for training\n",
    "    print(\"Computing updates ...\")\n",
    "    updates = lasagne.updates.adagrad(cost, all_params, LEARNING_RATE)\n",
    "\n",
    "    # Theano functions for training and computing cost\n",
    "    print(\"Compiling functions ...\")\n",
    "    train = theano.function([l_in.input_var, target_values], cost, updates=updates, allow_input_downcast=True)\n",
    "    compute_cost = theano.function([l_in.input_var, target_values], cost, allow_input_downcast=True)\n",
    "\n",
    "    # In order to generate text from the network, we need the probability distribution of the next character given\n",
    "    # the state of the network and the input (a seed).\n",
    "    # In order to produce the probability distribution of the prediction, we compile a function called probs. \n",
    "    \n",
    "    probs = theano.function([l_in.input_var],network_output,allow_input_downcast=True)\n",
    "\n",
    "    # The next function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "    # The phrase is set using the variable generation_phrase.\n",
    "    # The optional input \"N\" is used to set the number of characters of text to predict. \n",
    "\n",
    "    def try_it_out(N=200):\n",
    "        '''\n",
    "        This function uses the user-provided string \"generation_phrase\" and current state of the RNN generate text.\n",
    "        The function works in three steps:\n",
    "        1. It converts the string set in \"generation_phrase\" (which must be over SEQ_LENGTH characters long) \n",
    "           to encoded format. We use the gen_data function for this. By providing the string and asking for a single batch,\n",
    "           we are converting the first SEQ_LENGTH characters into encoded form. \n",
    "        2. We then use the LSTM to predict the next character and store it in a (dynamic) list sample_ix. This is done by using the 'probs'\n",
    "           function which was compiled above. Simply put, given the output, we compute the probabilities of the target and pick the one \n",
    "           with the highest predicted probability. \n",
    "        3. Once this character has been predicted, we construct a new sequence using all but first characters of the \n",
    "           provided string and the predicted character. This sequence is then used to generate yet another character.\n",
    "           This process continues for \"N\" characters. \n",
    "        To make this clear, let us again look at a concrete example. \n",
    "        Assume that SEQ_LENGTH = 5 and generation_phrase = \"The quick brown fox jumps\". \n",
    "        We initially encode the first 5 characters ('T','h','e',' ','q'). The next character is then predicted (as explained in step 2). \n",
    "        Assume that this character was 'J'. We then construct a new sequence using the last 4 (=SEQ_LENGTH-1) characters of the previous\n",
    "        sequence ('h','e',' ','q') , and the predicted letter 'J'. This new sequence is then used to compute the next character and \n",
    "        the process continues.\n",
    "        '''\n",
    "\n",
    "        assert(len(generation_phrase)>=SEQ_LENGTH)\n",
    "        sample_ix = []\n",
    "        x,_ = gen_data(len(generation_phrase)-SEQ_LENGTH, 1, generation_phrase,0)\n",
    "\n",
    "        for i in range(N):\n",
    "            # Pick the character that got assigned the highest probability\n",
    "            ix = np.argmax(probs(x).ravel())\n",
    "            # Alternatively, to sample from the distribution instead:\n",
    "            # ix = np.random.choice(np.arange(vocab_size), p=probs(x).ravel())\n",
    "            sample_ix.append(ix)\n",
    "            x[:,0:SEQ_LENGTH-1,:] = x[:,1:,:]\n",
    "            x[:,SEQ_LENGTH-1,:] = 0\n",
    "            x[0,SEQ_LENGTH-1,sample_ix[-1]] = 1. \n",
    "\n",
    "        random_snippet = generation_phrase + ''.join(ix_to_char[ix] for ix in sample_ix)    \n",
    "        print(\"----\\n %s \\n----\" % random_snippet)\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"Training ...\")\n",
    "    print(\"Seed used for text generation is: \" + generation_phrase)\n",
    "    p = 0\n",
    "    try:\n",
    "        for it in xrange(data_size * num_epochs / BATCH_SIZE):\n",
    "            try_it_out() # Generate text using the p^th character as the start. \n",
    "            \n",
    "            avg_cost = 0;\n",
    "            for _ in range(PRINT_FREQ):\n",
    "                x,y = gen_data(p)\n",
    "                \n",
    "                #print(p)\n",
    "                p += SEQ_LENGTH + BATCH_SIZE - 1 \n",
    "                if(p+BATCH_SIZE+SEQ_LENGTH >= data_size):\n",
    "                    print('Carriage Return')\n",
    "                    p = 0;\n",
    "                \n",
    "\n",
    "                avg_cost += train(x, y)\n",
    "            print(\"Epoch {} average loss = {}\".format(it*1.0*PRINT_FREQ/data_size*BATCH_SIZE, avg_cost / PRINT_FREQ))\n",
    "                    \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n",
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "Training ...\n",
      "Seed used for text generation is: The quick brown fox jumps\n",
      "----\n",
      " The quick brown fox jumpsXqq����kkkkqq66))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))) \n",
      "----"
     ]
    }
   ],
   "source": [
    "main(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
